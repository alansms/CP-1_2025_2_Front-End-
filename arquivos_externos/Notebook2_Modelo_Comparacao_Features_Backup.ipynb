{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Comparação de Modelos KMeans - Sinopse vs Todas as Features\n",
        "\n",
        "**Objetivo:** Comparar o desempenho do modelo KMeans utilizando apenas sinopses vetorizadas versus utilizando todas as features disponíveis\n",
        "\n",
        "**Desenvolvido por:** [Nome dos integrantes do grupo]\n",
        "\n",
        "**Data:** 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importação das Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurações para garantir que os prints funcionem\n",
        "import sys\n",
        "import os\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Configurar matplotlib para funcionar em notebooks\n",
        "import matplotlib\n",
        "matplotlib.use('inline')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "\n",
        "# Configurações do pandas para melhor display\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# Configurações do matplotlib\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Funções de display melhoradas\n",
        "import IPython\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "def print_and_display(text):\n",
        "    \"\"\"Função que faz print e também display para garantir visibilidade\"\"\"\n",
        "    print(text)\n",
        "    display(HTML(f\"<div style='background-color: #f0f0f0; padding: 10px; border-left: 4px solid #007acc;'>{text}</div>\"))\n",
        "\n",
        "def display_df(df, title=\"DataFrame\"):\n",
        "    \"\"\"Função para display de DataFrames com título\"\"\"\n",
        "    display(HTML(f\"<h4>{title}</h4>\"))\n",
        "    display(df)\n",
        "\n",
        "print_and_display(\"✅ Bibliotecas importadas com sucesso!\")\n",
        "print_and_display(f\"Python version: {sys.version}\")\n",
        "print_and_display(f\"Pandas version: {pd.__version__}\")\n",
        "print_and_display(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento e Preparação dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dataset (use o arquivo gerado pelo Notebook 1 ou o dataset original)\n",
        "try:\n",
        "    # Tentar carregar o dataset com clusters do Notebook 1\n",
        "    df = pd.read_csv('imdb_top250_with_clusters.csv', sep=';')\n",
        "    print(\"Dataset com clusters carregado com sucesso!\")\n",
        "except:\n",
        "    try:\n",
        "        # Tentar carregar o dataset original\n",
        "        df = pd.read_csv('all_movies.csv', sep=';')\n",
        "        print(\"Dataset original carregado com sucesso!\")\n",
        "    except:\n",
        "        print(\"Erro ao carregar dataset. Verifique se os arquivos existem.\")\n",
        "        # Criar dataset de exemplo para demonstração\n",
        "        df = pd.DataFrame({\n",
        "            'title_pt': ['Filme 1', 'Filme 2', 'Filme 3'],\n",
        "            'title_en': ['Movie 1', 'Movie 2', 'Movie 3'],\n",
        "            'year': [2020, 2021, 2022],\n",
        "            'rating': [8.5, 7.8, 9.2],\n",
        "            'genre': ['Action', 'Drama', 'Comedy'],\n",
        "            'sinopse': ['Ação emocionante', 'Drama tocante', 'Comédia divertida']\n",
        "        })\n",
        "\n",
        "print(f\"Shape do dataset: {df.shape}\")\n",
        "print(f\"Colunas: {list(df.columns)}\")\n",
        "print(\"\\nPrimeiras linhas:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparação dos dados\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Converter tipos de dados\n",
        "df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce')\n",
        "df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
        "\n",
        "# Remover linhas com valores críticos nulos\n",
        "df_clean = df_clean.dropna(subset=['title_en', 'sinopse'])\n",
        "\n",
        "# Preencher valores nulos\n",
        "df_clean['genre'] = df_clean['genre'].fillna('Unknown')\n",
        "\n",
        "# Limpeza de texto (se não foi feita no Notebook 1)\n",
        "if 'sinopse_clean' not in df_clean.columns:\n",
        "    import re\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    \n",
        "    try:\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except:\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
        "    \n",
        "    def clean_text(text):\n",
        "        if pd.isna(text) or text == 'N/A':\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        words = text.split()\n",
        "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "        return ' '.join(words)\n",
        "    \n",
        "    df_clean['sinopse_clean'] = df_clean['sinopse'].apply(clean_text)\n",
        "\n",
        "# Remover filmes sem sinopse válida\n",
        "df_clean = df_clean[df_clean['sinopse_clean'].str.len() > 10]\n",
        "\n",
        "print(f\"Dataset após limpeza: {df_clean.shape}\")\n",
        "print(f\"Valores nulos restantes:\")\n",
        "print(df_clean.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Modelo 1: KMeans com Apenas Sinopses Vetorizadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Apenas sinopses vetorizadas\n",
        "print(\"=== MODELO 1: APENAS SINOPSES VETORIZADAS ===\\n\")\n",
        "\n",
        "# Aplicar TF-IDF\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    min_df=2,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(df_clean['sinopse_clean'])\n",
        "print(f\"Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
        "\n",
        "# Treinar KMeans\n",
        "kmeans_model1 = KMeans(\n",
        "    n_clusters=5,\n",
        "    random_state=42,\n",
        "    n_init=10,\n",
        "    max_iter=300\n",
        ")\n",
        "\n",
        "cluster_labels_model1 = kmeans_model1.fit_predict(X_tfidf)\n",
        "df_clean['cluster_model1'] = cluster_labels_model1\n",
        "\n",
        "# Calcular métricas\n",
        "silhouette_model1 = silhouette_score(X_tfidf, cluster_labels_model1)\n",
        "calinski_model1 = calinski_harabasz_score(X_tfidf.toarray(), cluster_labels_model1)\n",
        "davies_bouldin_model1 = davies_bouldin_score(X_tfidf.toarray(), cluster_labels_model1)\n",
        "\n",
        "print(f\"Distribuição dos clusters (Modelo 1):\")\n",
        "print(df_clean['cluster_model1'].value_counts().sort_index())\n",
        "print(f\"\\nMétricas do Modelo 1:\")\n",
        "print(f\"Silhouette Score: {silhouette_model1:.3f}\")\n",
        "print(f\"Calinski-Harabasz Score: {calinski_model1:.3f}\")\n",
        "print(f\"Davies-Bouldin Score: {davies_bouldin_model1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modelo 2: KMeans com Todas as Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Todas as features\n",
        "print(\"=== MODELO 2: TODAS AS FEATURES ===\\n\")\n",
        "\n",
        "# Preparar features numéricas\n",
        "df_features = df_clean.copy()\n",
        "\n",
        "# Codificar gênero\n",
        "le_genre = LabelEncoder()\n",
        "df_features['genre_encoded'] = le_genre.fit_transform(df_features['genre'])\n",
        "\n",
        "# Criar features adicionais\n",
        "df_features['word_count'] = df_features['sinopse_clean'].apply(lambda x: len(x.split()))\n",
        "df_features['char_count'] = df_features['sinopse_clean'].apply(lambda x: len(x))\n",
        "\n",
        "# Normalizar features numéricas\n",
        "scaler = StandardScaler()\n",
        "numeric_features = ['year', 'rating', 'genre_encoded', 'word_count', 'char_count']\n",
        "df_features[numeric_features] = scaler.fit_transform(df_features[numeric_features])\n",
        "\n",
        "# Combinar features numéricas com TF-IDF\n",
        "X_numeric = df_features[numeric_features].values\n",
        "X_combined = np.hstack([X_numeric, X_tfidf.toarray()])\n",
        "\n",
        "print(f\"Shape das features numéricas: {X_numeric.shape}\")\n",
        "print(f\"Shape das features combinadas: {X_combined.shape}\")\n",
        "\n",
        "# Treinar KMeans\n",
        "kmeans_model2 = KMeans(\n",
        "    n_clusters=5,\n",
        "    random_state=42,\n",
        "    n_init=10,\n",
        "    max_iter=300\n",
        ")\n",
        "\n",
        "cluster_labels_model2 = kmeans_model2.fit_predict(X_combined)\n",
        "df_clean['cluster_model2'] = cluster_labels_model2\n",
        "\n",
        "# Calcular métricas\n",
        "silhouette_model2 = silhouette_score(X_combined, cluster_labels_model2)\n",
        "calinski_model2 = calinski_harabasz_score(X_combined, cluster_labels_model2)\n",
        "davies_bouldin_model2 = davies_bouldin_score(X_combined, cluster_labels_model2)\n",
        "\n",
        "print(f\"Distribuição dos clusters (Modelo 2):\")\n",
        "print(df_clean['cluster_model2'].value_counts().sort_index())\n",
        "print(f\"\\nMétricas do Modelo 2:\")\n",
        "print(f\"Silhouette Score: {silhouette_model2:.3f}\")\n",
        "print(f\"Calinski-Harabasz Score: {calinski_model2:.3f}\")\n",
        "print(f\"Davies-Bouldin Score: {davies_bouldin_model2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparação dos Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparação das métricas\n",
        "print(\"=== COMPARAÇÃO DAS MÉTRICAS ===\\n\")\n",
        "\n",
        "comparison_data = {\n",
        "    'Métrica': ['Silhouette Score', 'Calinski-Harabasz Score', 'Davies-Bouldin Score'],\n",
        "    'Modelo 1 (Apenas Sinopse)': [silhouette_model1, calinski_model1, davies_bouldin_model1],\n",
        "    'Modelo 2 (Todas Features)': [silhouette_model2, calinski_model2, davies_bouldin_model2]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.round(3))\n",
        "\n",
        "# Visualização da comparação\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = ['Silhouette Score', 'Calinski-Harabasz Score', 'Davies-Bouldin Score']\n",
        "model1_values = [silhouette_model1, calinski_model1, davies_bouldin_model1]\n",
        "model2_values = [silhouette_model2, calinski_model2, davies_bouldin_model2]\n",
        "\n",
        "for i, (metric, val1, val2) in enumerate(zip(metrics, model1_values, model2_values)):\n",
        "    axes[i].bar(['Modelo 1', 'Modelo 2'], [val1, val2], color=['#FF6B6B', '#4ECDC4'])\n",
        "    axes[i].set_title(metric)\n",
        "    axes[i].set_ylabel('Valor')\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    axes[i].text(0, val1 + max(val1, val2) * 0.01, f'{val1:.3f}', ha='center', va='bottom')\n",
        "    axes[i].text(1, val2 + max(val1, val2) * 0.01, f'{val2:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determinar qual modelo é melhor para cada métrica\n",
        "print(\"\\n=== ANÁLISE DOS RESULTADOS ===\")\n",
        "print(\"Silhouette Score (maior é melhor):\")\n",
        "if silhouette_model1 > silhouette_model2:\n",
        "    print(f\"  Modelo 1 é melhor: {silhouette_model1:.3f} vs {silhouette_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 é melhor: {silhouette_model2:.3f} vs {silhouette_model1:.3f}\")\n",
        "\n",
        "print(\"\\nCalinski-Harabasz Score (maior é melhor):\")\n",
        "if calinski_model1 > calinski_model2:\n",
        "    print(f\"  Modelo 1 é melhor: {calinski_model1:.3f} vs {calinski_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 é melhor: {calinski_model2:.3f} vs {calinski_model1:.3f}\")\n",
        "\n",
        "print(\"\\nDavies-Bouldin Score (menor é melhor):\")\n",
        "if davies_bouldin_model1 < davies_bouldin_model2:\n",
        "    print(f\"  Modelo 1 é melhor: {davies_bouldin_model1:.3f} vs {davies_bouldin_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 é melhor: {davies_bouldin_model2:.3f} vs {davies_bouldin_model1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise detalhada dos clusters de cada modelo\n",
        "print(\"=== ANÁLISE DETALHADA DOS CLUSTERS ===\\n\")\n",
        "\n",
        "# Função para analisar clusters\n",
        "def analyze_clusters(df, cluster_col, model_name):\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for cluster_id in sorted(df[cluster_col].unique()):\n",
        "        cluster_data = df[df[cluster_col] == cluster_id]\n",
        "        \n",
        "        print(f\"\\nCluster {cluster_id}:\")\n",
        "        print(f\"  Número de filmes: {len(cluster_data)}\")\n",
        "        print(f\"  Rating médio: {cluster_data['rating'].mean():.2f}\")\n",
        "        print(f\"  Ano médio: {cluster_data['year'].mean():.0f}\")\n",
        "        \n",
        "        # Gêneros mais comuns\n",
        "        top_genres = cluster_data['genre'].value_counts().head(3)\n",
        "        print(f\"  Gêneros principais: {', '.join([f'{genre} ({count})' for genre, count in top_genres.items()])}\")\n",
        "        \n",
        "        # Filmes mais bem avaliados\n",
        "        top_movies = cluster_data.nlargest(2, 'rating')[['title_en', 'rating']]\n",
        "        print(f\"  Filmes top: {', '.join([f'{movie[0]} ({movie[1]:.1f})' for _, movie in top_movies.iterrows()])}\")\n",
        "\n",
        "# Analisar clusters do Modelo 1\n",
        "analyze_clusters(df_clean, 'cluster_model1', 'MODELO 1 (Apenas Sinopse)')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Analisar clusters do Modelo 2\n",
        "analyze_clusters(df_clean, 'cluster_model2', 'MODELO 2 (Todas Features)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizações Comparativas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizações comparativas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Distribuição dos clusters - Modelo 1\n",
        "axes[0, 0].bar(df_clean['cluster_model1'].value_counts().sort_index().index, \n",
        "               df_clean['cluster_model1'].value_counts().sort_index().values,\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[0, 0].set_title('Modelo 1: Distribuição dos Clusters')\n",
        "axes[0, 0].set_xlabel('Cluster')\n",
        "axes[0, 0].set_ylabel('Número de Filmes')\n",
        "\n",
        "# 2. Distribuição dos clusters - Modelo 2\n",
        "axes[0, 1].bar(df_clean['cluster_model2'].value_counts().sort_index().index, \n",
        "               df_clean['cluster_model2'].value_counts().sort_index().values,\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[0, 1].set_title('Modelo 2: Distribuição dos Clusters')\n",
        "axes[0, 1].set_xlabel('Cluster')\n",
        "axes[0, 1].set_ylabel('Número de Filmes')\n",
        "\n",
        "# 3. Rating médio por cluster - Comparação\n",
        "rating_model1 = df_clean.groupby('cluster_model1')['rating'].mean()\n",
        "rating_model2 = df_clean.groupby('cluster_model2')['rating'].mean()\n",
        "\n",
        "x = np.arange(len(rating_model1))\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 2].bar(x - width/2, rating_model1.values, width, label='Modelo 1', color='#FF6B6B')\n",
        "axes[0, 2].bar(x + width/2, rating_model2.values, width, label='Modelo 2', color='#4ECDC4')\n",
        "axes[0, 2].set_title('Rating Médio por Cluster')\n",
        "axes[0, 2].set_xlabel('Cluster')\n",
        "axes[0, 2].set_ylabel('Rating Médio')\n",
        "axes[0, 2].set_xticks(x)\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# 4. Ano médio por cluster - Comparação\n",
        "year_model1 = df_clean.groupby('cluster_model1')['year'].mean()\n",
        "year_model2 = df_clean.groupby('cluster_model2')['year'].mean()\n",
        "\n",
        "axes[1, 0].bar(x - width/2, year_model1.values, width, label='Modelo 1', color='#FF6B6B')\n",
        "axes[1, 0].bar(x + width/2, year_model2.values, width, label='Modelo 2', color='#4ECDC4')\n",
        "axes[1, 0].set_title('Ano Médio por Cluster')\n",
        "axes[1, 0].set_xlabel('Cluster')\n",
        "axes[1, 0].set_ylabel('Ano Médio')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. Gêneros por cluster - Modelo 1\n",
        "genre_cluster1 = pd.crosstab(df_clean['genre'], df_clean['cluster_model1'])\n",
        "genre_cluster1_pct = genre_cluster1.div(genre_cluster1.sum(axis=0), axis=1) * 100\n",
        "top_genres = df_clean['genre'].value_counts().head(5).index\n",
        "genre_cluster1_pct.loc[top_genres].plot(kind='bar', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Modelo 1: Gêneros por Cluster')\n",
        "axes[1, 1].set_xlabel('Gênero')\n",
        "axes[1, 1].set_ylabel('Percentual (%)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 6. Gêneros por cluster - Modelo 2\n",
        "genre_cluster2 = pd.crosstab(df_clean['genre'], df_clean['cluster_model2'])\n",
        "genre_cluster2_pct = genre_cluster2.div(genre_cluster2.sum(axis=0), axis=1) * 100\n",
        "genre_cluster2_pct.loc[top_genres].plot(kind='bar', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Modelo 2: Gêneros por Cluster')\n",
        "axes[1, 2].set_xlabel('Gênero')\n",
        "axes[1, 2].set_ylabel('Percentual (%)')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Escolha do Melhor Modelo e Justificativa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise final e escolha do melhor modelo\n",
        "print(\"=== ANÁLISE FINAL E ESCOLHA DO MELHOR MODELO ===\\n\")\n",
        "\n",
        "# Calcular score composto (média normalizada das métricas)\n",
        "def calculate_composite_score(silhouette, calinski, davies_bouldin):\n",
        "    \"\"\"\n",
        "    Calcula um score composto normalizando as métricas\n",
        "    Silhouette e Calinski-Harabasz: maior é melhor\n",
        "    Davies-Bouldin: menor é melhor\n",
        "    \"\"\"\n",
        "    # Normalizar Silhouette (0-1)\n",
        "    silhouette_norm = max(0, silhouette)  # Silhouette pode ser negativo\n",
        "    \n",
        "    # Normalizar Calinski-Harabasz (assumindo range 0-1000)\n",
        "    calinski_norm = min(1, calinski / 1000)\n",
        "    \n",
        "    # Normalizar Davies-Bouldin (inverter, menor é melhor)\n",
        "    davies_bouldin_norm = max(0, 1 - davies_bouldin / 5)  # Assumindo range 0-5\n",
        "    \n",
        "    # Score composto (média ponderada)\n",
        "    composite_score = (silhouette_norm * 0.4 + calinski_norm * 0.3 + davies_bouldin_norm * 0.3)\n",
        "    return composite_score\n",
        "\n",
        "score_model1 = calculate_composite_score(silhouette_model1, calinski_model1, davies_bouldin_model1)\n",
        "score_model2 = calculate_composite_score(silhouette_model2, calinski_model2, davies_bouldin_model2)\n",
        "\n",
        "print(f\"Score Composto Modelo 1: {score_model1:.3f}\")\n",
        "print(f\"Score Composto Modelo 2: {score_model2:.3f}\")\n",
        "\n",
        "# Determinar o melhor modelo\n",
        "if score_model1 > score_model2:\n",
        "    best_model = \"Modelo 1 (Apenas Sinopse)\"\n",
        "    best_score = score_model1\n",
        "    print(f\"\\n🏆 MELHOR MODELO: {best_model}\")\n",
        "else:\n",
        "    best_model = \"Modelo 2 (Todas Features)\"\n",
        "    best_score = score_model2\n",
        "    print(f\"\\n🏆 MELHOR MODELO: {best_model}\")\n",
        "\n",
        "print(f\"Score: {best_score:.3f}\")\n",
        "\n",
        "# Análise de consistência dos clusters\n",
        "print(f\"\\n=== ANÁLISE DE CONSISTÊNCIA ===\")\n",
        "\n",
        "# Verificar se os clusters são consistentes entre os modelos\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "ari_score = adjusted_rand_score(df_clean['cluster_model1'], df_clean['cluster_model2'])\n",
        "print(f\"Adjusted Rand Index (consistência entre modelos): {ari_score:.3f}\")\n",
        "\n",
        "if ari_score > 0.5:\n",
        "    print(\"Os modelos produzem clusters relativamente consistentes\")\n",
        "elif ari_score > 0.2:\n",
        "    print(\"Os modelos produzem clusters parcialmente consistentes\")\n",
        "else:\n",
        "    print(\"Os modelos produzem clusters muito diferentes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Justificativa da Escolha\n",
        "\n",
        "**Critérios de Avaliação:**\n",
        "\n",
        "1. **Métricas Quantitativas:**\n",
        "   - **Silhouette Score:** Mede a qualidade da separação dos clusters (maior é melhor)\n",
        "   - **Calinski-Harabasz Score:** Mede a razão entre dispersão entre clusters e dentro dos clusters (maior é melhor)\n",
        "   - **Davies-Bouldin Score:** Mede a qualidade da separação baseada na distância intra-cluster (menor é melhor)\n",
        "\n",
        "2. **Interpretabilidade:**\n",
        "   - Capacidade de interpretar e explicar os clusters encontrados\n",
        "   - Coerência com características conhecidas dos filmes\n",
        "\n",
        "3. **Aplicabilidade Prática:**\n",
        "   - Facilidade de implementação em sistemas de recomendação\n",
        "   - Robustez para novos dados\n",
        "\n",
        "**Justificativa da Escolha:**\n",
        "\n",
        "[Baseado nos resultados obtidos, justificar qual modelo foi escolhido e por quê]\n",
        "\n",
        "**Vantagens do Modelo Escolhido:**\n",
        "- [Listar vantagens específicas]\n",
        "\n",
        "**Limitações do Modelo Escolhido:**\n",
        "- [Listar limitações e como contorná-las]\n",
        "\n",
        "**Recomendações para Implementação:**\n",
        "- [Sugerir melhorias e próximos passos]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar resultados da comparação\n",
        "print(\"Salvando resultados da comparação...\")\n",
        "\n",
        "# Salvar DataFrame com ambos os modelos\n",
        "df_clean.to_csv('imdb_comparison_results.csv', index=False, sep=';')\n",
        "print(\"Resultados salvos em 'imdb_comparison_results.csv'\")\n",
        "\n",
        "# Criar resumo da comparação\n",
        "comparison_summary = {\n",
        "    'Modelo': ['Modelo 1 (Apenas Sinopse)', 'Modelo 2 (Todas Features)'],\n",
        "    'Silhouette_Score': [silhouette_model1, silhouette_model2],\n",
        "    'Calinski_Harabasz_Score': [calinski_model1, calinski_model2],\n",
        "    'Davies_Bouldin_Score': [davies_bouldin_model1, davies_bouldin_model2],\n",
        "    'Composite_Score': [score_model1, score_model2],\n",
        "    'Melhor_Modelo': [best_model == 'Modelo 1 (Apenas Sinopse)', best_model == 'Modelo 2 (Todas Features)']\n",
        "}\n",
        "\n",
        "comparison_summary_df = pd.DataFrame(comparison_summary)\n",
        "comparison_summary_df.to_csv('model_comparison_summary.csv', index=False, sep=';')\n",
        "print(\"Resumo da comparação salvo em 'model_comparison_summary.csv'\")\n",
        "\n",
        "print(\"\\n=== RESUMO FINAL ===\")\n",
        "print(f\"Total de filmes analisados: {len(df_clean)}\")\n",
        "print(f\"Modelo escolhido: {best_model}\")\n",
        "print(f\"Score composto: {best_score:.3f}\")\n",
        "print(f\"Consistência entre modelos (ARI): {ari_score:.3f}\")\n",
        "\n",
        "print(\"\\nMétricas finais:\")\n",
        "print(comparison_summary_df.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
