{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Compara√ß√£o de Modelos KMeans - Sinopse vs Todas as Features\n",
        "\n",
        "**Objetivo:** Comparar o desempenho do modelo KMeans utilizando apenas sinopses vetorizadas versus utilizando todas as features dispon√≠veis\n",
        "\n",
        "**Desenvolvido por:** [Nome dos integrantes do grupo]\n",
        "\n",
        "**Data:** 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importa√ß√£o das Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura√ß√µes para garantir que os prints funcionem\n",
        "import sys\n",
        "import os\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Configurar matplotlib para funcionar em notebooks\n",
        "import matplotlib\n",
        "matplotlib.use('inline')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "\n",
        "# Configura√ß√µes do pandas para melhor display\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# Configura√ß√µes do matplotlib\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Fun√ß√µes de display melhoradas\n",
        "import IPython\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "def print_and_display(text):\n",
        "    \"\"\"Fun√ß√£o que faz print e tamb√©m display para garantir visibilidade\"\"\"\n",
        "    print(text)\n",
        "    display(HTML(f\"<div style='background-color: #f0f0f0; padding: 10px; border-left: 4px solid #007acc;'>{text}</div>\"))\n",
        "\n",
        "def display_df(df, title=\"DataFrame\"):\n",
        "    \"\"\"Fun√ß√£o para display de DataFrames com t√≠tulo\"\"\"\n",
        "    display(HTML(f\"<h4>{title}</h4>\"))\n",
        "    display(df)\n",
        "\n",
        "print_and_display(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "print_and_display(f\"Python version: {sys.version}\")\n",
        "print_and_display(f\"Pandas version: {pd.__version__}\")\n",
        "print_and_display(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento e Prepara√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dataset (use o arquivo gerado pelo Notebook 1 ou o dataset original)\n",
        "try:\n",
        "    # Tentar carregar o dataset com clusters do Notebook 1\n",
        "    df = pd.read_csv('imdb_top250_with_clusters.csv', sep=';')\n",
        "    print(\"Dataset com clusters carregado com sucesso!\")\n",
        "except:\n",
        "    try:\n",
        "        # Tentar carregar o dataset original\n",
        "        df = pd.read_csv('all_movies.csv', sep=';')\n",
        "        print(\"Dataset original carregado com sucesso!\")\n",
        "    except:\n",
        "        print(\"Erro ao carregar dataset. Verifique se os arquivos existem.\")\n",
        "        # Criar dataset de exemplo para demonstra√ß√£o\n",
        "        df = pd.DataFrame({\n",
        "            'title_pt': ['Filme 1', 'Filme 2', 'Filme 3'],\n",
        "            'title_en': ['Movie 1', 'Movie 2', 'Movie 3'],\n",
        "            'year': [2020, 2021, 2022],\n",
        "            'rating': [8.5, 7.8, 9.2],\n",
        "            'genre': ['Action', 'Drama', 'Comedy'],\n",
        "            'sinopse': ['A√ß√£o emocionante', 'Drama tocante', 'Com√©dia divertida']\n",
        "        })\n",
        "\n",
        "print(f\"Shape do dataset: {df.shape}\")\n",
        "print(f\"Colunas: {list(df.columns)}\")\n",
        "print(\"\\nPrimeiras linhas:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepara√ß√£o dos dados\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Converter tipos de dados\n",
        "df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce')\n",
        "df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
        "\n",
        "# Remover linhas com valores cr√≠ticos nulos\n",
        "df_clean = df_clean.dropna(subset=['title_en', 'sinopse'])\n",
        "\n",
        "# Preencher valores nulos\n",
        "df_clean['genre'] = df_clean['genre'].fillna('Unknown')\n",
        "\n",
        "# Limpeza de texto (se n√£o foi feita no Notebook 1)\n",
        "if 'sinopse_clean' not in df_clean.columns:\n",
        "    import re\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    \n",
        "    try:\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except:\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
        "    \n",
        "    def clean_text(text):\n",
        "        if pd.isna(text) or text == 'N/A':\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        words = text.split()\n",
        "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "        return ' '.join(words)\n",
        "    \n",
        "    df_clean['sinopse_clean'] = df_clean['sinopse'].apply(clean_text)\n",
        "\n",
        "# Remover filmes sem sinopse v√°lida\n",
        "df_clean = df_clean[df_clean['sinopse_clean'].str.len() > 10]\n",
        "\n",
        "print(f\"Dataset ap√≥s limpeza: {df_clean.shape}\")\n",
        "print(f\"Valores nulos restantes:\")\n",
        "print(df_clean.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Modelo 1: KMeans com Apenas Sinopses Vetorizadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Apenas sinopses vetorizadas\n",
        "print(\"=== MODELO 1: APENAS SINOPSES VETORIZADAS ===\\n\")\n",
        "\n",
        "# Aplicar TF-IDF\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    min_df=2,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(df_clean['sinopse_clean'])\n",
        "print(f\"Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
        "\n",
        "# Treinar KMeans\n",
        "kmeans_model1 = KMeans(\n",
        "    n_clusters=5,\n",
        "    random_state=42,\n",
        "    n_init=10,\n",
        "    max_iter=300\n",
        ")\n",
        "\n",
        "cluster_labels_model1 = kmeans_model1.fit_predict(X_tfidf)\n",
        "df_clean['cluster_model1'] = cluster_labels_model1\n",
        "\n",
        "# Calcular m√©tricas\n",
        "silhouette_model1 = silhouette_score(X_tfidf, cluster_labels_model1)\n",
        "calinski_model1 = calinski_harabasz_score(X_tfidf.toarray(), cluster_labels_model1)\n",
        "davies_bouldin_model1 = davies_bouldin_score(X_tfidf.toarray(), cluster_labels_model1)\n",
        "\n",
        "print(f\"Distribui√ß√£o dos clusters (Modelo 1):\")\n",
        "print(df_clean['cluster_model1'].value_counts().sort_index())\n",
        "print(f\"\\nM√©tricas do Modelo 1:\")\n",
        "print(f\"Silhouette Score: {silhouette_model1:.3f}\")\n",
        "print(f\"Calinski-Harabasz Score: {calinski_model1:.3f}\")\n",
        "print(f\"Davies-Bouldin Score: {davies_bouldin_model1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modelo 2: KMeans com Todas as Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Todas as features\n",
        "print(\"=== MODELO 2: TODAS AS FEATURES ===\\n\")\n",
        "\n",
        "# Preparar features num√©ricas\n",
        "df_features = df_clean.copy()\n",
        "\n",
        "# Codificar g√™nero\n",
        "le_genre = LabelEncoder()\n",
        "df_features['genre_encoded'] = le_genre.fit_transform(df_features['genre'])\n",
        "\n",
        "# Criar features adicionais\n",
        "df_features['word_count'] = df_features['sinopse_clean'].apply(lambda x: len(x.split()))\n",
        "df_features['char_count'] = df_features['sinopse_clean'].apply(lambda x: len(x))\n",
        "\n",
        "# Normalizar features num√©ricas\n",
        "scaler = StandardScaler()\n",
        "numeric_features = ['year', 'rating', 'genre_encoded', 'word_count', 'char_count']\n",
        "df_features[numeric_features] = scaler.fit_transform(df_features[numeric_features])\n",
        "\n",
        "# Combinar features num√©ricas com TF-IDF\n",
        "X_numeric = df_features[numeric_features].values\n",
        "X_combined = np.hstack([X_numeric, X_tfidf.toarray()])\n",
        "\n",
        "print(f\"Shape das features num√©ricas: {X_numeric.shape}\")\n",
        "print(f\"Shape das features combinadas: {X_combined.shape}\")\n",
        "\n",
        "# Treinar KMeans\n",
        "kmeans_model2 = KMeans(\n",
        "    n_clusters=5,\n",
        "    random_state=42,\n",
        "    n_init=10,\n",
        "    max_iter=300\n",
        ")\n",
        "\n",
        "cluster_labels_model2 = kmeans_model2.fit_predict(X_combined)\n",
        "df_clean['cluster_model2'] = cluster_labels_model2\n",
        "\n",
        "# Calcular m√©tricas\n",
        "silhouette_model2 = silhouette_score(X_combined, cluster_labels_model2)\n",
        "calinski_model2 = calinski_harabasz_score(X_combined, cluster_labels_model2)\n",
        "davies_bouldin_model2 = davies_bouldin_score(X_combined, cluster_labels_model2)\n",
        "\n",
        "print(f\"Distribui√ß√£o dos clusters (Modelo 2):\")\n",
        "print(df_clean['cluster_model2'].value_counts().sort_index())\n",
        "print(f\"\\nM√©tricas do Modelo 2:\")\n",
        "print(f\"Silhouette Score: {silhouette_model2:.3f}\")\n",
        "print(f\"Calinski-Harabasz Score: {calinski_model2:.3f}\")\n",
        "print(f\"Davies-Bouldin Score: {davies_bouldin_model2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compara√ß√£o dos Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o das m√©tricas\n",
        "print(\"=== COMPARA√á√ÉO DAS M√âTRICAS ===\\n\")\n",
        "\n",
        "comparison_data = {\n",
        "    'M√©trica': ['Silhouette Score', 'Calinski-Harabasz Score', 'Davies-Bouldin Score'],\n",
        "    'Modelo 1 (Apenas Sinopse)': [silhouette_model1, calinski_model1, davies_bouldin_model1],\n",
        "    'Modelo 2 (Todas Features)': [silhouette_model2, calinski_model2, davies_bouldin_model2]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.round(3))\n",
        "\n",
        "# Visualiza√ß√£o da compara√ß√£o\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = ['Silhouette Score', 'Calinski-Harabasz Score', 'Davies-Bouldin Score']\n",
        "model1_values = [silhouette_model1, calinski_model1, davies_bouldin_model1]\n",
        "model2_values = [silhouette_model2, calinski_model2, davies_bouldin_model2]\n",
        "\n",
        "for i, (metric, val1, val2) in enumerate(zip(metrics, model1_values, model2_values)):\n",
        "    axes[i].bar(['Modelo 1', 'Modelo 2'], [val1, val2], color=['#FF6B6B', '#4ECDC4'])\n",
        "    axes[i].set_title(metric)\n",
        "    axes[i].set_ylabel('Valor')\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    axes[i].text(0, val1 + max(val1, val2) * 0.01, f'{val1:.3f}', ha='center', va='bottom')\n",
        "    axes[i].text(1, val2 + max(val1, val2) * 0.01, f'{val2:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determinar qual modelo √© melhor para cada m√©trica\n",
        "print(\"\\n=== AN√ÅLISE DOS RESULTADOS ===\")\n",
        "print(\"Silhouette Score (maior √© melhor):\")\n",
        "if silhouette_model1 > silhouette_model2:\n",
        "    print(f\"  Modelo 1 √© melhor: {silhouette_model1:.3f} vs {silhouette_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 √© melhor: {silhouette_model2:.3f} vs {silhouette_model1:.3f}\")\n",
        "\n",
        "print(\"\\nCalinski-Harabasz Score (maior √© melhor):\")\n",
        "if calinski_model1 > calinski_model2:\n",
        "    print(f\"  Modelo 1 √© melhor: {calinski_model1:.3f} vs {calinski_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 √© melhor: {calinski_model2:.3f} vs {calinski_model1:.3f}\")\n",
        "\n",
        "print(\"\\nDavies-Bouldin Score (menor √© melhor):\")\n",
        "if davies_bouldin_model1 < davies_bouldin_model2:\n",
        "    print(f\"  Modelo 1 √© melhor: {davies_bouldin_model1:.3f} vs {davies_bouldin_model2:.3f}\")\n",
        "else:\n",
        "    print(f\"  Modelo 2 √© melhor: {davies_bouldin_model2:.3f} vs {davies_bouldin_model1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise detalhada dos clusters de cada modelo\n",
        "print(\"=== AN√ÅLISE DETALHADA DOS CLUSTERS ===\\n\")\n",
        "\n",
        "# Fun√ß√£o para analisar clusters\n",
        "def analyze_clusters(df, cluster_col, model_name):\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for cluster_id in sorted(df[cluster_col].unique()):\n",
        "        cluster_data = df[df[cluster_col] == cluster_id]\n",
        "        \n",
        "        print(f\"\\nCluster {cluster_id}:\")\n",
        "        print(f\"  N√∫mero de filmes: {len(cluster_data)}\")\n",
        "        print(f\"  Rating m√©dio: {cluster_data['rating'].mean():.2f}\")\n",
        "        print(f\"  Ano m√©dio: {cluster_data['year'].mean():.0f}\")\n",
        "        \n",
        "        # G√™neros mais comuns\n",
        "        top_genres = cluster_data['genre'].value_counts().head(3)\n",
        "        print(f\"  G√™neros principais: {', '.join([f'{genre} ({count})' for genre, count in top_genres.items()])}\")\n",
        "        \n",
        "        # Filmes mais bem avaliados\n",
        "        top_movies = cluster_data.nlargest(2, 'rating')[['title_en', 'rating']]\n",
        "        print(f\"  Filmes top: {', '.join([f'{movie[0]} ({movie[1]:.1f})' for _, movie in top_movies.iterrows()])}\")\n",
        "\n",
        "# Analisar clusters do Modelo 1\n",
        "analyze_clusters(df_clean, 'cluster_model1', 'MODELO 1 (Apenas Sinopse)')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Analisar clusters do Modelo 2\n",
        "analyze_clusters(df_clean, 'cluster_model2', 'MODELO 2 (Todas Features)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualiza√ß√µes Comparativas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√µes comparativas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Distribui√ß√£o dos clusters - Modelo 1\n",
        "axes[0, 0].bar(df_clean['cluster_model1'].value_counts().sort_index().index, \n",
        "               df_clean['cluster_model1'].value_counts().sort_index().values,\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[0, 0].set_title('Modelo 1: Distribui√ß√£o dos Clusters')\n",
        "axes[0, 0].set_xlabel('Cluster')\n",
        "axes[0, 0].set_ylabel('N√∫mero de Filmes')\n",
        "\n",
        "# 2. Distribui√ß√£o dos clusters - Modelo 2\n",
        "axes[0, 1].bar(df_clean['cluster_model2'].value_counts().sort_index().index, \n",
        "               df_clean['cluster_model2'].value_counts().sort_index().values,\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[0, 1].set_title('Modelo 2: Distribui√ß√£o dos Clusters')\n",
        "axes[0, 1].set_xlabel('Cluster')\n",
        "axes[0, 1].set_ylabel('N√∫mero de Filmes')\n",
        "\n",
        "# 3. Rating m√©dio por cluster - Compara√ß√£o\n",
        "rating_model1 = df_clean.groupby('cluster_model1')['rating'].mean()\n",
        "rating_model2 = df_clean.groupby('cluster_model2')['rating'].mean()\n",
        "\n",
        "x = np.arange(len(rating_model1))\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 2].bar(x - width/2, rating_model1.values, width, label='Modelo 1', color='#FF6B6B')\n",
        "axes[0, 2].bar(x + width/2, rating_model2.values, width, label='Modelo 2', color='#4ECDC4')\n",
        "axes[0, 2].set_title('Rating M√©dio por Cluster')\n",
        "axes[0, 2].set_xlabel('Cluster')\n",
        "axes[0, 2].set_ylabel('Rating M√©dio')\n",
        "axes[0, 2].set_xticks(x)\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# 4. Ano m√©dio por cluster - Compara√ß√£o\n",
        "year_model1 = df_clean.groupby('cluster_model1')['year'].mean()\n",
        "year_model2 = df_clean.groupby('cluster_model2')['year'].mean()\n",
        "\n",
        "axes[1, 0].bar(x - width/2, year_model1.values, width, label='Modelo 1', color='#FF6B6B')\n",
        "axes[1, 0].bar(x + width/2, year_model2.values, width, label='Modelo 2', color='#4ECDC4')\n",
        "axes[1, 0].set_title('Ano M√©dio por Cluster')\n",
        "axes[1, 0].set_xlabel('Cluster')\n",
        "axes[1, 0].set_ylabel('Ano M√©dio')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. G√™neros por cluster - Modelo 1\n",
        "genre_cluster1 = pd.crosstab(df_clean['genre'], df_clean['cluster_model1'])\n",
        "genre_cluster1_pct = genre_cluster1.div(genre_cluster1.sum(axis=0), axis=1) * 100\n",
        "top_genres = df_clean['genre'].value_counts().head(5).index\n",
        "genre_cluster1_pct.loc[top_genres].plot(kind='bar', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Modelo 1: G√™neros por Cluster')\n",
        "axes[1, 1].set_xlabel('G√™nero')\n",
        "axes[1, 1].set_ylabel('Percentual (%)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 6. G√™neros por cluster - Modelo 2\n",
        "genre_cluster2 = pd.crosstab(df_clean['genre'], df_clean['cluster_model2'])\n",
        "genre_cluster2_pct = genre_cluster2.div(genre_cluster2.sum(axis=0), axis=1) * 100\n",
        "genre_cluster2_pct.loc[top_genres].plot(kind='bar', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Modelo 2: G√™neros por Cluster')\n",
        "axes[1, 2].set_xlabel('G√™nero')\n",
        "axes[1, 2].set_ylabel('Percentual (%)')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Escolha do Melhor Modelo e Justificativa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise final e escolha do melhor modelo\n",
        "print(\"=== AN√ÅLISE FINAL E ESCOLHA DO MELHOR MODELO ===\\n\")\n",
        "\n",
        "# Calcular score composto (m√©dia normalizada das m√©tricas)\n",
        "def calculate_composite_score(silhouette, calinski, davies_bouldin):\n",
        "    \"\"\"\n",
        "    Calcula um score composto normalizando as m√©tricas\n",
        "    Silhouette e Calinski-Harabasz: maior √© melhor\n",
        "    Davies-Bouldin: menor √© melhor\n",
        "    \"\"\"\n",
        "    # Normalizar Silhouette (0-1)\n",
        "    silhouette_norm = max(0, silhouette)  # Silhouette pode ser negativo\n",
        "    \n",
        "    # Normalizar Calinski-Harabasz (assumindo range 0-1000)\n",
        "    calinski_norm = min(1, calinski / 1000)\n",
        "    \n",
        "    # Normalizar Davies-Bouldin (inverter, menor √© melhor)\n",
        "    davies_bouldin_norm = max(0, 1 - davies_bouldin / 5)  # Assumindo range 0-5\n",
        "    \n",
        "    # Score composto (m√©dia ponderada)\n",
        "    composite_score = (silhouette_norm * 0.4 + calinski_norm * 0.3 + davies_bouldin_norm * 0.3)\n",
        "    return composite_score\n",
        "\n",
        "score_model1 = calculate_composite_score(silhouette_model1, calinski_model1, davies_bouldin_model1)\n",
        "score_model2 = calculate_composite_score(silhouette_model2, calinski_model2, davies_bouldin_model2)\n",
        "\n",
        "print(f\"Score Composto Modelo 1: {score_model1:.3f}\")\n",
        "print(f\"Score Composto Modelo 2: {score_model2:.3f}\")\n",
        "\n",
        "# Determinar o melhor modelo\n",
        "if score_model1 > score_model2:\n",
        "    best_model = \"Modelo 1 (Apenas Sinopse)\"\n",
        "    best_score = score_model1\n",
        "    print(f\"\\nüèÜ MELHOR MODELO: {best_model}\")\n",
        "else:\n",
        "    best_model = \"Modelo 2 (Todas Features)\"\n",
        "    best_score = score_model2\n",
        "    print(f\"\\nüèÜ MELHOR MODELO: {best_model}\")\n",
        "\n",
        "print(f\"Score: {best_score:.3f}\")\n",
        "\n",
        "# An√°lise de consist√™ncia dos clusters\n",
        "print(f\"\\n=== AN√ÅLISE DE CONSIST√äNCIA ===\")\n",
        "\n",
        "# Verificar se os clusters s√£o consistentes entre os modelos\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "ari_score = adjusted_rand_score(df_clean['cluster_model1'], df_clean['cluster_model2'])\n",
        "print(f\"Adjusted Rand Index (consist√™ncia entre modelos): {ari_score:.3f}\")\n",
        "\n",
        "if ari_score > 0.5:\n",
        "    print(\"Os modelos produzem clusters relativamente consistentes\")\n",
        "elif ari_score > 0.2:\n",
        "    print(\"Os modelos produzem clusters parcialmente consistentes\")\n",
        "else:\n",
        "    print(\"Os modelos produzem clusters muito diferentes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Justificativa da Escolha\n",
        "\n",
        "**Crit√©rios de Avalia√ß√£o:**\n",
        "\n",
        "1. **M√©tricas Quantitativas:**\n",
        "   - **Silhouette Score:** Mede a qualidade da separa√ß√£o dos clusters (maior √© melhor)\n",
        "   - **Calinski-Harabasz Score:** Mede a raz√£o entre dispers√£o entre clusters e dentro dos clusters (maior √© melhor)\n",
        "   - **Davies-Bouldin Score:** Mede a qualidade da separa√ß√£o baseada na dist√¢ncia intra-cluster (menor √© melhor)\n",
        "\n",
        "2. **Interpretabilidade:**\n",
        "   - Capacidade de interpretar e explicar os clusters encontrados\n",
        "   - Coer√™ncia com caracter√≠sticas conhecidas dos filmes\n",
        "\n",
        "3. **Aplicabilidade Pr√°tica:**\n",
        "   - Facilidade de implementa√ß√£o em sistemas de recomenda√ß√£o\n",
        "   - Robustez para novos dados\n",
        "\n",
        "**Justificativa da Escolha:**\n",
        "\n",
        "[Baseado nos resultados obtidos, justificar qual modelo foi escolhido e por qu√™]\n",
        "\n",
        "**Vantagens do Modelo Escolhido:**\n",
        "- [Listar vantagens espec√≠ficas]\n",
        "\n",
        "**Limita√ß√µes do Modelo Escolhido:**\n",
        "- [Listar limita√ß√µes e como contorn√°-las]\n",
        "\n",
        "**Recomenda√ß√µes para Implementa√ß√£o:**\n",
        "- [Sugerir melhorias e pr√≥ximos passos]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar resultados da compara√ß√£o\n",
        "print(\"Salvando resultados da compara√ß√£o...\")\n",
        "\n",
        "# Salvar DataFrame com ambos os modelos\n",
        "df_clean.to_csv('imdb_comparison_results.csv', index=False, sep=';')\n",
        "print(\"Resultados salvos em 'imdb_comparison_results.csv'\")\n",
        "\n",
        "# Criar resumo da compara√ß√£o\n",
        "comparison_summary = {\n",
        "    'Modelo': ['Modelo 1 (Apenas Sinopse)', 'Modelo 2 (Todas Features)'],\n",
        "    'Silhouette_Score': [silhouette_model1, silhouette_model2],\n",
        "    'Calinski_Harabasz_Score': [calinski_model1, calinski_model2],\n",
        "    'Davies_Bouldin_Score': [davies_bouldin_model1, davies_bouldin_model2],\n",
        "    'Composite_Score': [score_model1, score_model2],\n",
        "    'Melhor_Modelo': [best_model == 'Modelo 1 (Apenas Sinopse)', best_model == 'Modelo 2 (Todas Features)']\n",
        "}\n",
        "\n",
        "comparison_summary_df = pd.DataFrame(comparison_summary)\n",
        "comparison_summary_df.to_csv('model_comparison_summary.csv', index=False, sep=';')\n",
        "print(\"Resumo da compara√ß√£o salvo em 'model_comparison_summary.csv'\")\n",
        "\n",
        "print(\"\\n=== RESUMO FINAL ===\")\n",
        "print(f\"Total de filmes analisados: {len(df_clean)}\")\n",
        "print(f\"Modelo escolhido: {best_model}\")\n",
        "print(f\"Score composto: {best_score:.3f}\")\n",
        "print(f\"Consist√™ncia entre modelos (ARI): {ari_score:.3f}\")\n",
        "\n",
        "print(\"\\nM√©tricas finais:\")\n",
        "print(comparison_summary_df.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
