{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: IMDb Top 250 Movies - Web Scraping e Clusterização KMeans\n",
    "\n",
    "**Objetivo:** Aprimorar o código de web scraping para obter dados dos 250 filmes do IMDb e treinar modelo KMeans com k=5\n",
    "\n",
    "**Desenvolvido por:** [Nome dos integrantes do grupo]\n",
    "\n",
    "**Data:** 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação do ambiente\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Max rows: {pd.get_option('display.max_rows')}\")\n",
    "print(f\"Max columns: {pd.get_option('display.max_columns')}\")\n",
    "\n",
    "# Verificar ambiente Jupyter\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        print(f\"Ambiente Jupyter detectado: {type(ipython).__name__}\")\n",
    "    else:\n",
    "        print(\"Ambiente Jupyter não detectado\")\n",
    "except ImportError:\n",
    "    print(\"IPython não disponível\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas necessárias (execute apenas se necessário)\n",
    "# !pip install requests beautifulsoup4 pandas numpy matplotlib seaborn plotly wordcloud scikit-learn nltk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('inline')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping - IMDb Top 250 Movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb_top250():\n",
    "    \"\"\"\n",
    "    Função para fazer web scraping dos top 250 filmes do IMDb\n",
    "    \"\"\"\n",
    "    url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movies_data = []\n",
    "        \n",
    "        table = soup.find('tbody', class_='lister-list')\n",
    "        if not table:\n",
    "            print(\"Tabela não encontrada!\")\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all('tr')\n",
    "        print(f\"Encontrados {len(rows)} filmes na página principal\")\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            try:\n",
    "                title_cell = row.find('td', class_='titleColumn')\n",
    "                if not title_cell:\n",
    "                    continue\n",
    "                    \n",
    "                title_link = title_cell.find('a')\n",
    "                title = title_link.text.strip() if title_link else \"N/A\"\n",
    "                \n",
    "                year_span = title_cell.find('span', class_='secondaryInfo')\n",
    "                year = year_span.text.strip('()') if year_span else \"N/A\"\n",
    "                \n",
    "                rating_cell = row.find('td', class_='ratingColumn imdbRating')\n",
    "                rating = rating_cell.find('strong').text.strip() if rating_cell and rating_cell.find('strong') else \"N/A\"\n",
    "                \n",
    "                movie_url = \"https://www.imdb.com\" + title_link['href'] if title_link else None\n",
    "                \n",
    "                print(f\"Processando filme {i+1}/250: {title} ({year})\")\n",
    "                \n",
    "                movie_details = scrape_movie_details(movie_url, headers) if movie_url else {}\n",
    "                \n",
    "                movie_data = {\n",
    "                    'rank': i + 1,\n",
    "                    'title_en': title,\n",
    "                    'year': year,\n",
    "                    'rating': rating,\n",
    "                    'genre': movie_details.get('genre', 'N/A'),\n",
    "                    'sinopse': movie_details.get('plot', 'N/A'),\n",
    "                    'director': movie_details.get('director', 'N/A'),\n",
    "                    'cast': movie_details.get('cast', 'N/A'),\n",
    "                    'duration': movie_details.get('duration', 'N/A')\n",
    "                }\n",
    "                \n",
    "                movies_data.append(movie_data)\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar filme {i+1}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return movies_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar a página: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_movie_details(movie_url, headers):\n",
    "    \"\"\"\n",
    "    Função para extrair detalhes adicionais de cada filme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        genre_element = soup.find('div', {'data-testid': 'genres'})\n",
    "        if genre_element:\n",
    "            genres = [span.text.strip() for span in genre_element.find_all('span')]\n",
    "            details['genre'] = ', '.join(genres)\n",
    "        \n",
    "        plot_element = soup.find('span', {'data-testid': 'plot-xl'})\n",
    "        if plot_element:\n",
    "            details['plot'] = plot_element.text.strip()\n",
    "        else:\n",
    "            plot_alt = soup.find('div', class_='summary_text')\n",
    "            if plot_alt:\n",
    "                details['plot'] = plot_alt.text.strip()\n",
    "        \n",
    "        director_element = soup.find('a', {'data-testid': 'title-pc-principal-credit'})\n",
    "        if director_element:\n",
    "            details['director'] = director_element.text.strip()\n",
    "        \n",
    "        duration_element = soup.find('li', {'data-testid': 'title-techspec_runtime'})\n",
    "        if duration_element:\n",
    "            details['duration'] = duration_element.find('div').text.strip()\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair detalhes do filme: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "print(\"Funções de web scraping definidas!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o web scraping\n",
    "print(\"Iniciando web scraping dos top 250 filmes do IMDb...\")\n",
    "print(\"Este processo pode levar alguns minutos...\")\n",
    "\n",
    "movies_data = scrape_imdb_top250()\n",
    "\n",
    "if movies_data:\n",
    "    print(f\"\\nWeb scraping concluído! {len(movies_data)} filmes extraídos.\")\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    \n",
    "    # Salvar em CSV\n",
    "    df.to_csv('imdb_top250_enhanced.csv', index=False, sep=';')\n",
    "    print(\"Dados salvos em 'imdb_top250_enhanced.csv'\")\n",
    "    \n",
    "    # Mostrar primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nShape do dataset: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"Erro no web scraping. Verifique sua conexão com a internet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset (descomente se necessário)\n",
    "# df = pd.read_csv('imdb_top250_enhanced.csv', sep=';')\n",
    "\n",
    "# Verificar se o DataFrame foi criado\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame não encontrado. Execute a célula anterior ou carregue um arquivo CSV.\")\n",
    "else:\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "    print(\"\\nPrimeiras linhas:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping Aprimorado - IMDb Top 250 Movies\n",
    "\n",
    "Vamos aprimorar o código de web scraping para extrair dados dos 250 filmes do IMDb de forma mais robusta e completa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb_top250():\n",
    "    \"\"\"\n",
    "    Função aprimorada para fazer web scraping dos top 250 filmes do IMDb\n",
    "    \"\"\"\n",
    "    url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "    \n",
    "    # Headers para simular um navegador real\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movies_data = []\n",
    "        \n",
    "        # Encontrar a tabela com os filmes\n",
    "        table = soup.find('tbody', class_='lister-list')\n",
    "        if not table:\n",
    "            print(\"Tabela não encontrada!\")\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all('tr')\n",
    "        print(f\"Encontrados {len(rows)} filmes na página principal\")\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            try:\n",
    "                # Extrair informações básicas\n",
    "                title_cell = row.find('td', class_='titleColumn')\n",
    "                if not title_cell:\n",
    "                    continue\n",
    "                    \n",
    "                # Título e ano\n",
    "                title_link = title_cell.find('a')\n",
    "                title = title_link.text.strip() if title_link else \"N/A\"\n",
    "                \n",
    "                year_span = title_cell.find('span', class_='secondaryInfo')\n",
    "                year = year_span.text.strip('()') if year_span else \"N/A\"\n",
    "                \n",
    "                # Rating\n",
    "                rating_cell = row.find('td', class_='ratingColumn imdbRating')\n",
    "                rating = rating_cell.find('strong').text.strip() if rating_cell and rating_cell.find('strong') else \"N/A\"\n",
    "                \n",
    "                # Link para página do filme\n",
    "                movie_url = \"https://www.imdb.com\" + title_link['href'] if title_link else None\n",
    "                \n",
    "                print(f\"Processando filme {i+1}/250: {title} ({year})\")\n",
    "                \n",
    "                # Fazer scraping da página individual do filme para obter mais detalhes\n",
    "                movie_details = scrape_movie_details(movie_url, headers) if movie_url else {}\n",
    "                \n",
    "                movie_data = {\n",
    "                    'rank': i + 1,\n",
    "                    'title_en': title,\n",
    "                    'year': year,\n",
    "                    'rating': rating,\n",
    "                    'genre': movie_details.get('genre', 'N/A'),\n",
    "                    'sinopse': movie_details.get('plot', 'N/A'),\n",
    "                    'director': movie_details.get('director', 'N/A'),\n",
    "                    'cast': movie_details.get('cast', 'N/A'),\n",
    "                    'duration': movie_details.get('duration', 'N/A')\n",
    "                }\n",
    "                \n",
    "                movies_data.append(movie_data)\n",
    "                \n",
    "                # Pausa para evitar sobrecarga do servidor\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar filme {i+1}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return movies_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar a página: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_movie_details(movie_url, headers):\n",
    "    \"\"\"\n",
    "    Função para extrair detalhes adicionais de cada filme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        # Gênero\n",
    "        genre_element = soup.find('div', {'data-testid': 'genres'})\n",
    "        if genre_element:\n",
    "            genres = [span.text.strip() for span in genre_element.find_all('span')]\n",
    "            details['genre'] = ', '.join(genres)\n",
    "        \n",
    "        # Sinopse/Plot\n",
    "        plot_element = soup.find('span', {'data-testid': 'plot-xl'})\n",
    "        if plot_element:\n",
    "            details['plot'] = plot_element.text.strip()\n",
    "        else:\n",
    "            # Tentar encontrar sinopse alternativa\n",
    "            plot_alt = soup.find('div', class_='summary_text')\n",
    "            if plot_alt:\n",
    "                details['plot'] = plot_alt.text.strip()\n",
    "        \n",
    "        # Diretor\n",
    "        director_element = soup.find('a', {'data-testid': 'title-pc-principal-credit'})\n",
    "        if director_element:\n",
    "            details['director'] = director_element.text.strip()\n",
    "        \n",
    "        # Duração\n",
    "        duration_element = soup.find('li', {'data-testid': 'title-techspec_runtime'})\n",
    "        if duration_element:\n",
    "            details['duration'] = duration_element.find('div').text.strip()\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair detalhes do filme: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "print(\"Funções de web scraping definidas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Executando o Web Scraping\n",
    "\n",
    "**Atenção:** O web scraping pode levar alguns minutos para ser concluído, pois fazemos uma pausa entre cada requisição para respeitar o servidor do IMDb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o web scraping\n",
    "print(\"Iniciando web scraping dos top 250 filmes do IMDb...\")\n",
    "print(\"Este processo pode levar alguns minutos...\")\n",
    "\n",
    "movies_data = scrape_imdb_top250()\n",
    "\n",
    "if movies_data:\n",
    "    print(f\"\\nWeb scraping concluído! {len(movies_data)} filmes extraídos.\")\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    \n",
    "    # Salvar em CSV\n",
    "    df.to_csv('imdb_top250_enhanced.csv', index=False, sep=';')\n",
    "    print(\"Dados salvos em 'imdb_top250_enhanced.csv'\")\n",
    "    \n",
    "    # Mostrar primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nShape do dataset: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"Erro no web scraping. Verifique sua conexão com a internet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Carregando Dataset (Alternativa)\n",
    "\n",
    "Caso o web scraping não funcione ou você queira usar dados pré-existentes, pode carregar o dataset do arquivo CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset (descomente se necessário)\n",
    "# df = pd.read_csv('imdb_top250_enhanced.csv', sep=';')\n",
    "\n",
    "# Verificar se o DataFrame foi criado\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame não encontrado. Execute a célula anterior ou carregue um arquivo CSV.\")\n",
    "else:\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "    print(\"\\nPrimeiras linhas:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análise Exploratória dos Dados (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informações básicas do dataset\n",
    "print(\"=== INFORMAÇÕES BÁSICAS DO DATASET ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Colunas: {list(df.columns)}\")\n",
    "print(\"\\n=== TIPOS DE DADOS ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== VALORES NULOS ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n=== ESTATÍSTICAS DESCRITIVAS ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza e preparação dos dados\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Converter tipos de dados\n",
    "df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce')\n",
    "df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
    "\n",
    "# Remover linhas com valores críticos nulos\n",
    "df_clean = df_clean.dropna(subset=['title_en', 'sinopse'])\n",
    "\n",
    "# Preencher valores nulos em gênero com 'Unknown'\n",
    "df_clean['genre'] = df_clean['genre'].fillna('Unknown')\n",
    "\n",
    "print(f\"Dataset após limpeza: {df_clean.shape}\")\n",
    "print(f\"Valores nulos restantes:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações exploratórias\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Distribuição de gêneros\n",
    "plt.subplot(2, 3, 1)\n",
    "genre_counts = df_clean['genre'].value_counts().head(10)\n",
    "plt.bar(range(len(genre_counts)), genre_counts.values)\n",
    "plt.xticks(range(len(genre_counts)), genre_counts.index, rotation=45, ha='right')\n",
    "plt.title('Top 10 Gêneros')\n",
    "plt.ylabel('Quantidade de Filmes')\n",
    "\n",
    "# 2. Distribuição de anos\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df_clean['year'].dropna(), bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribuição de Anos de Lançamento')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Frequência')\n",
    "\n",
    "# 3. Distribuição de ratings\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df_clean['rating'].dropna(), bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribuição de Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequência')\n",
    "\n",
    "# 4. Rating vs Ano\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df_clean['year'], df_clean['rating'], alpha=0.6)\n",
    "plt.title('Rating vs Ano de Lançamento')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Rating')\n",
    "\n",
    "# 5. Top 10 filmes por rating\n",
    "plt.subplot(2, 3, 5)\n",
    "top_movies = df_clean.nlargest(10, 'rating')[['title_en', 'rating']]\n",
    "plt.barh(range(len(top_movies)), top_movies['rating'])\n",
    "plt.yticks(range(len(top_movies)), [title[:30] + '...' if len(title) > 30 else title for title in top_movies['title_en']])\n",
    "plt.title('Top 10 Filmes por Rating')\n",
    "plt.xlabel('Rating')\n",
    "\n",
    "# 6. Filmes por década\n",
    "plt.subplot(2, 3, 6)\n",
    "df_clean['decade'] = (df_clean['year'] // 10) * 10\n",
    "decade_counts = df_clean['decade'].value_counts().sort_index()\n",
    "plt.bar(decade_counts.index, decade_counts.values)\n",
    "plt.title('Filmes por Década')\n",
    "plt.xlabel('Década')\n",
    "plt.ylabel('Quantidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pré-processamento de Texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar stopwords do NLTK\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"Stopwords em inglês carregadas com sucesso!\")\n",
    "except:\n",
    "    print(\"Erro ao carregar stopwords. Usando lista básica.\")\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
    "\n",
    "# Função para limpeza de texto\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Função para limpar e pré-processar texto\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == 'N/A':\n",
    "        return \"\"\n",
    "    \n",
    "    # Converter para minúsculas\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remover caracteres especiais e números\n",
    "    import re\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar limpeza nas sinopses\n",
    "print(\"Aplicando limpeza de texto nas sinopses...\")\n",
    "df_clean['sinopse_clean'] = df_clean['sinopse'].apply(clean_text)\n",
    "\n",
    "# Remover filmes sem sinopse válida\n",
    "df_clean = df_clean[df_clean['sinopse_clean'].str.len() > 10]\n",
    "\n",
    "print(f\"Dataset após limpeza de texto: {df_clean.shape}\")\n",
    "print(f\"Exemplo de sinopse limpa:\")\n",
    "print(df_clean['sinopse_clean'].iloc[0][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar TF-IDF\n",
    "print(\"Aplicando TF-IDF nas sinopses...\")\n",
    "\n",
    "# Configurar TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limitar número de features\n",
    "    min_df=2,           # Palavra deve aparecer em pelo menos 2 documentos\n",
    "    max_df=0.8,         # Palavra não pode aparecer em mais de 80% dos documentos\n",
    "    ngram_range=(1, 2)  # Usar unigramas e bigramas\n",
    ")\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(df_clean['sinopse_clean'])\n",
    "\n",
    "print(f\"Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
    "print(f\"Número de features (palavras): {X_tfidf.shape[1]}\")\n",
    "print(f\"Número de documentos (filmes): {X_tfidf.shape[0]}\")\n",
    "\n",
    "# Mostrar algumas palavras mais importantes\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nPrimeiras 20 features: {feature_names[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo KMeans com k=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo KMeans com k=5\n",
    "print(\"Treinando modelo KMeans com k=5...\")\n",
    "\n",
    "# Configurar e treinar o modelo\n",
    "kmeans = KMeans(\n",
    "    n_clusters=5,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "cluster_labels = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Adicionar labels dos clusters ao DataFrame\n",
    "df_clean['cluster'] = cluster_labels\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "print(f\"Distribuição dos clusters:\")\n",
    "print(df_clean['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Calcular silhouette score\n",
    "silhouette_avg = silhouette_score(X_tfidf, cluster_labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise dos Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da distribuição dos clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 1. Distribuição dos clusters\n",
    "plt.subplot(2, 2, 1)\n",
    "cluster_counts = df_clean['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_counts.index, cluster_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Distribuição dos Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Número de Filmes')\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    plt.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 2. Gêneros por cluster\n",
    "plt.subplot(2, 2, 2)\n",
    "# Criar tabela de contingência\n",
    "genre_cluster = pd.crosstab(df_clean['genre'], df_clean['cluster'])\n",
    "# Normalizar por cluster\n",
    "genre_cluster_pct = genre_cluster.div(genre_cluster.sum(axis=0), axis=1) * 100\n",
    "# Plotar apenas os gêneros mais comuns\n",
    "top_genres = df_clean['genre'].value_counts().head(5).index\n",
    "genre_cluster_pct.loc[top_genres].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Distribuição de Gêneros por Cluster')\n",
    "plt.xlabel('Gênero')\n",
    "plt.ylabel('Percentual (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# 3. Rating médio por cluster\n",
    "plt.subplot(2, 2, 3)\n",
    "rating_by_cluster = df_clean.groupby('cluster')['rating'].mean()\n",
    "plt.bar(rating_by_cluster.index, rating_by_cluster.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Rating Médio por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Rating Médio')\n",
    "for i, v in enumerate(rating_by_cluster.values):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Ano médio por cluster\n",
    "plt.subplot(2, 2, 4)\n",
    "year_by_cluster = df_clean.groupby('cluster')['year'].mean()\n",
    "plt.bar(year_by_cluster.index, year_by_cluster.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Ano Médio por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Ano Médio')\n",
    "for i, v in enumerate(year_by_cluster.values):\n",
    "    plt.text(i, v + 1, f'{v:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise detalhada de cada cluster\n",
    "print(\"=== ANÁLISE DETALHADA DOS CLUSTERS ===\\n\")\n",
    "\n",
    "for cluster_id in sorted(df_clean['cluster'].unique()):\n",
    "    cluster_data = df_clean[df_clean['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"--- CLUSTER {cluster_id} ---\")\n",
    "    print(f\"Número de filmes: {len(cluster_data)}\")\n",
    "    print(f\"Rating médio: {cluster_data['rating'].mean():.2f}\")\n",
    "    print(f\"Ano médio: {cluster_data['year'].mean():.0f}\")\n",
    "    \n",
    "    # Gêneros mais comuns\n",
    "    top_genres = cluster_data['genre'].value_counts().head(3)\n",
    "    print(f\"Gêneros mais comuns: {', '.join([f'{genre} ({count})' for genre, count in top_genres.items()])}\")\n",
    "    \n",
    "    # Filmes mais bem avaliados\n",
    "    top_movies = cluster_data.nlargest(3, 'rating')[['title_en', 'rating', 'year']]\n",
    "    print(\"Filmes mais bem avaliados:\")\n",
    "    for _, movie in top_movies.iterrows():\n",
    "        print(f\"  - {movie['title_en']} ({movie['year']}) - Rating: {movie['rating']}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🆘 Guia de Solução de Problemas\n",
    "\n",
    "### **Problema: Não consigo ver os prints**\n",
    "\n",
    "**Soluções em ordem de prioridade:**\n",
    "\n",
    "1. **Reiniciar o Kernel**\n",
    "   - Vá em `Kernel` → `Restart & Clear Output`\n",
    "   - Execute as células novamente\n",
    "\n",
    "2. **Verificar o Ambiente**\n",
    "   - Certifique-se de que está usando Jupyter Notebook ou Jupyter Lab\n",
    "   - Verifique se o kernel Python está ativo (canto superior direito)\n",
    "\n",
    "3. **Usar as Funções de Display**\n",
    "   - Use `print_and_display()` em vez de `print()`\n",
    "   - Use `display_df()` para DataFrames\n",
    "\n",
    "4. **Verificar Configurações**\n",
    "   - Execute a célula de diagnóstico no início do notebook\n",
    "   - Verifique se todas as bibliotecas foram importadas corretamente\n",
    "\n",
    "5. **Alternativas**\n",
    "   - Use `display()` do IPython\n",
    "   - Use `IPython.display.HTML()` para textos formatados\n",
    "   - Salve os resultados em arquivos CSV e abra externamente\n",
    "\n",
    "### **Problema: Erro de importação de bibliotecas**\n",
    "\n",
    "**Soluções:**\n",
    "- Execute: `!pip install [nome_da_biblioteca]`\n",
    "- Reinicie o kernel após instalação\n",
    "- Verifique se está no ambiente correto\n",
    "\n",
    "### **Problema: Web scraping não funciona**\n",
    "\n",
    "**Soluções:**\n",
    "- Verifique sua conexão com a internet\n",
    "- Use a versão alternativa com dados simulados\n",
    "- Execute o código em horários de menor tráfego\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Nuvem de Palavras por Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar nuvem de palavras por cluster\n",
    "def create_wordcloud_for_cluster(cluster_id, df_data, column='sinopse_clean'):\n",
    "    \"\"\"\n",
    "    Cria nuvem de palavras para um cluster específico\n",
    "    \"\"\"\n",
    "    cluster_data = df_data[df_data['cluster'] == cluster_id]\n",
    "    text = ' '.join(cluster_data[column].astype(str))\n",
    "    \n",
    "    if len(text.strip()) == 0:\n",
    "        print(f\"Cluster {cluster_id}: Sem texto disponível\")\n",
    "        return\n",
    "    \n",
    "    # Criar nuvem de palavras\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate(text)\n",
    "    \n",
    "    # Plotar\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Cluster {cluster_id} - Palavras Mais Frequentes\\n({len(cluster_data)} filmes)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# Criar nuvens de palavras para cada cluster\n",
    "print(\"Criando nuvens de palavras para cada cluster...\")\n",
    "for cluster_id in sorted(df_clean['cluster'].unique()):\n",
    "    create_wordcloud_for_cluster(cluster_id, df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusões e Insights\n",
    "\n",
    "### 7.1 Análise dos Resultados do Modelo KMeans (k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights e Conclusões:**\n",
    "\n",
    "1. **Distribuição dos Clusters:**\n",
    "   - O modelo KMeans com k=5 conseguiu agrupar os filmes de forma relativamente equilibrada\n",
    "   - Cada cluster representa um perfil distinto de filmes baseado nas sinopses\n",
    "\n",
    "2. **Características dos Clusters:**\n",
    "   - **Cluster 0:** [Descrever características baseadas na análise]\n",
    "   - **Cluster 1:** [Descrever características baseadas na análise]\n",
    "   - **Cluster 2:** [Descrever características baseadas na análise]\n",
    "   - **Cluster 3:** [Descrever características baseadas na análise]\n",
    "   - **Cluster 4:** [Descrever características baseadas na análise]\n",
    "\n",
    "3. **Padrões Identificados:**\n",
    "   - [Identificar padrões nos gêneros, anos, ratings por cluster]\n",
    "   - [Analisar se há correlação entre características dos filmes e clusters]\n",
    "\n",
    "4. **Qualidade do Modelo:**\n",
    "   - Silhouette Score: [Valor obtido]\n",
    "   - [Avaliar se o score indica boa separação dos clusters]\n",
    "\n",
    "5. **Aplicação Prática:**\n",
    "   - O modelo pode ser usado para recomendar filmes similares baseados nas sinopses\n",
    "   - Cada cluster representa um \"perfil\" de filme que pode ser usado para personalização\n",
    "\n",
    "**Limitações:**\n",
    "- O modelo considera apenas as sinopses, ignorando outras características importantes\n",
    "- A qualidade das sinopses extraídas pode variar\n",
    "- Alguns filmes podem não se encaixar perfeitamente em nenhum cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados finais\n",
    "print(\"Salvando resultados finais...\")\n",
    "\n",
    "# Salvar DataFrame com clusters\n",
    "df_clean.to_csv('imdb_top250_with_clusters.csv', index=False, sep=';')\n",
    "print(\"Dataset com clusters salvo em 'imdb_top250_with_clusters.csv'\")\n",
    "\n",
    "# Salvar resumo dos clusters\n",
    "cluster_summary = df_clean.groupby('cluster').agg({\n",
    "    'title_en': 'count',\n",
    "    'rating': 'mean',\n",
    "    'year': 'mean',\n",
    "    'genre': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "}).round(2)\n",
    "\n",
    "cluster_summary.columns = ['Num_Filmes', 'Rating_Medio', 'Ano_Medio', 'Genero_Principal']\n",
    "cluster_summary.to_csv('cluster_summary.csv', sep=';')\n",
    "print(\"Resumo dos clusters salvo em 'cluster_summary.csv'\")\n",
    "\n",
    "print(\"\\n=== RESUMO FINAL ===\")\n",
    "print(f\"Total de filmes analisados: {len(df_clean)}\")\n",
    "print(f\"Número de clusters: {df_clean['cluster'].nunique()}\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(\"\\nDistribuição dos clusters:\")\n",
    "print(df_clean['cluster'].value_counts().sort_index())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
