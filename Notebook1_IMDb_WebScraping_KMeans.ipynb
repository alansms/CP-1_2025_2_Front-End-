{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: IMDb Top 250 Movies - Web Scraping e Clusteriza√ß√£o KMeans\n",
    "\n",
    "**Objetivo:** Aprimorar o c√≥digo de web scraping para obter dados dos 250 filmes do IMDb e treinar modelo KMeans com k=5\n",
    "\n",
    "**Desenvolvido por:** [Nome dos integrantes do grupo]\n",
    "\n",
    "**Data:** 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o do ambiente\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Max rows: {pd.get_option('display.max_rows')}\")\n",
    "print(f\"Max columns: {pd.get_option('display.max_columns')}\")\n",
    "\n",
    "# Verificar ambiente Jupyter\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        print(f\"Ambiente Jupyter detectado: {type(ipython).__name__}\")\n",
    "    else:\n",
    "        print(\"Ambiente Jupyter n√£o detectado\")\n",
    "except ImportError:\n",
    "    print(\"IPython n√£o dispon√≠vel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o das Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das bibliotecas necess√°rias (execute apenas se necess√°rio)\n",
    "# !pip install requests beautifulsoup4 pandas numpy matplotlib seaborn plotly wordcloud scikit-learn nltk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('inline')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping - IMDb Top 250 Movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb_top250():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para fazer web scraping dos top 250 filmes do IMDb\n",
    "    \"\"\"\n",
    "    url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movies_data = []\n",
    "        \n",
    "        table = soup.find('tbody', class_='lister-list')\n",
    "        if not table:\n",
    "            print(\"Tabela n√£o encontrada!\")\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all('tr')\n",
    "        print(f\"Encontrados {len(rows)} filmes na p√°gina principal\")\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            try:\n",
    "                title_cell = row.find('td', class_='titleColumn')\n",
    "                if not title_cell:\n",
    "                    continue\n",
    "                    \n",
    "                title_link = title_cell.find('a')\n",
    "                title = title_link.text.strip() if title_link else \"N/A\"\n",
    "                \n",
    "                year_span = title_cell.find('span', class_='secondaryInfo')\n",
    "                year = year_span.text.strip('()') if year_span else \"N/A\"\n",
    "                \n",
    "                rating_cell = row.find('td', class_='ratingColumn imdbRating')\n",
    "                rating = rating_cell.find('strong').text.strip() if rating_cell and rating_cell.find('strong') else \"N/A\"\n",
    "                \n",
    "                movie_url = \"https://www.imdb.com\" + title_link['href'] if title_link else None\n",
    "                \n",
    "                print(f\"Processando filme {i+1}/250: {title} ({year})\")\n",
    "                \n",
    "                movie_details = scrape_movie_details(movie_url, headers) if movie_url else {}\n",
    "                \n",
    "                movie_data = {\n",
    "                    'rank': i + 1,\n",
    "                    'title_en': title,\n",
    "                    'year': year,\n",
    "                    'rating': rating,\n",
    "                    'genre': movie_details.get('genre', 'N/A'),\n",
    "                    'sinopse': movie_details.get('plot', 'N/A'),\n",
    "                    'director': movie_details.get('director', 'N/A'),\n",
    "                    'cast': movie_details.get('cast', 'N/A'),\n",
    "                    'duration': movie_details.get('duration', 'N/A')\n",
    "                }\n",
    "                \n",
    "                movies_data.append(movie_data)\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar filme {i+1}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return movies_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar a p√°gina: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_movie_details(movie_url, headers):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para extrair detalhes adicionais de cada filme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        genre_element = soup.find('div', {'data-testid': 'genres'})\n",
    "        if genre_element:\n",
    "            genres = [span.text.strip() for span in genre_element.find_all('span')]\n",
    "            details['genre'] = ', '.join(genres)\n",
    "        \n",
    "        plot_element = soup.find('span', {'data-testid': 'plot-xl'})\n",
    "        if plot_element:\n",
    "            details['plot'] = plot_element.text.strip()\n",
    "        else:\n",
    "            plot_alt = soup.find('div', class_='summary_text')\n",
    "            if plot_alt:\n",
    "                details['plot'] = plot_alt.text.strip()\n",
    "        \n",
    "        director_element = soup.find('a', {'data-testid': 'title-pc-principal-credit'})\n",
    "        if director_element:\n",
    "            details['director'] = director_element.text.strip()\n",
    "        \n",
    "        duration_element = soup.find('li', {'data-testid': 'title-techspec_runtime'})\n",
    "        if duration_element:\n",
    "            details['duration'] = duration_element.find('div').text.strip()\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair detalhes do filme: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "print(\"Fun√ß√µes de web scraping definidas!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o web scraping\n",
    "print(\"Iniciando web scraping dos top 250 filmes do IMDb...\")\n",
    "print(\"Este processo pode levar alguns minutos...\")\n",
    "\n",
    "movies_data = scrape_imdb_top250()\n",
    "\n",
    "if movies_data:\n",
    "    print(f\"\\nWeb scraping conclu√≠do! {len(movies_data)} filmes extra√≠dos.\")\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    \n",
    "    # Salvar em CSV\n",
    "    df.to_csv('imdb_top250_enhanced.csv', index=False, sep=';')\n",
    "    print(\"Dados salvos em 'imdb_top250_enhanced.csv'\")\n",
    "    \n",
    "    # Mostrar primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nShape do dataset: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"Erro no web scraping. Verifique sua conex√£o com a internet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset (descomente se necess√°rio)\n",
    "# df = pd.read_csv('imdb_top250_enhanced.csv', sep=';')\n",
    "\n",
    "# Verificar se o DataFrame foi criado\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame n√£o encontrado. Execute a c√©lula anterior ou carregue um arquivo CSV.\")\n",
    "else:\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "    print(\"\\nPrimeiras linhas:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping Aprimorado - IMDb Top 250 Movies\n",
    "\n",
    "Vamos aprimorar o c√≥digo de web scraping para extrair dados dos 250 filmes do IMDb de forma mais robusta e completa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb_top250():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o aprimorada para fazer web scraping dos top 250 filmes do IMDb\n",
    "    \"\"\"\n",
    "    url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "    \n",
    "    # Headers para simular um navegador real\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movies_data = []\n",
    "        \n",
    "        # Encontrar a tabela com os filmes\n",
    "        table = soup.find('tbody', class_='lister-list')\n",
    "        if not table:\n",
    "            print(\"Tabela n√£o encontrada!\")\n",
    "            return None\n",
    "            \n",
    "        rows = table.find_all('tr')\n",
    "        print(f\"Encontrados {len(rows)} filmes na p√°gina principal\")\n",
    "        \n",
    "        for i, row in enumerate(rows):\n",
    "            try:\n",
    "                # Extrair informa√ß√µes b√°sicas\n",
    "                title_cell = row.find('td', class_='titleColumn')\n",
    "                if not title_cell:\n",
    "                    continue\n",
    "                    \n",
    "                # T√≠tulo e ano\n",
    "                title_link = title_cell.find('a')\n",
    "                title = title_link.text.strip() if title_link else \"N/A\"\n",
    "                \n",
    "                year_span = title_cell.find('span', class_='secondaryInfo')\n",
    "                year = year_span.text.strip('()') if year_span else \"N/A\"\n",
    "                \n",
    "                # Rating\n",
    "                rating_cell = row.find('td', class_='ratingColumn imdbRating')\n",
    "                rating = rating_cell.find('strong').text.strip() if rating_cell and rating_cell.find('strong') else \"N/A\"\n",
    "                \n",
    "                # Link para p√°gina do filme\n",
    "                movie_url = \"https://www.imdb.com\" + title_link['href'] if title_link else None\n",
    "                \n",
    "                print(f\"Processando filme {i+1}/250: {title} ({year})\")\n",
    "                \n",
    "                # Fazer scraping da p√°gina individual do filme para obter mais detalhes\n",
    "                movie_details = scrape_movie_details(movie_url, headers) if movie_url else {}\n",
    "                \n",
    "                movie_data = {\n",
    "                    'rank': i + 1,\n",
    "                    'title_en': title,\n",
    "                    'year': year,\n",
    "                    'rating': rating,\n",
    "                    'genre': movie_details.get('genre', 'N/A'),\n",
    "                    'sinopse': movie_details.get('plot', 'N/A'),\n",
    "                    'director': movie_details.get('director', 'N/A'),\n",
    "                    'cast': movie_details.get('cast', 'N/A'),\n",
    "                    'duration': movie_details.get('duration', 'N/A')\n",
    "                }\n",
    "                \n",
    "                movies_data.append(movie_data)\n",
    "                \n",
    "                # Pausa para evitar sobrecarga do servidor\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar filme {i+1}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return movies_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar a p√°gina: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_movie_details(movie_url, headers):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para extrair detalhes adicionais de cada filme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        # G√™nero\n",
    "        genre_element = soup.find('div', {'data-testid': 'genres'})\n",
    "        if genre_element:\n",
    "            genres = [span.text.strip() for span in genre_element.find_all('span')]\n",
    "            details['genre'] = ', '.join(genres)\n",
    "        \n",
    "        # Sinopse/Plot\n",
    "        plot_element = soup.find('span', {'data-testid': 'plot-xl'})\n",
    "        if plot_element:\n",
    "            details['plot'] = plot_element.text.strip()\n",
    "        else:\n",
    "            # Tentar encontrar sinopse alternativa\n",
    "            plot_alt = soup.find('div', class_='summary_text')\n",
    "            if plot_alt:\n",
    "                details['plot'] = plot_alt.text.strip()\n",
    "        \n",
    "        # Diretor\n",
    "        director_element = soup.find('a', {'data-testid': 'title-pc-principal-credit'})\n",
    "        if director_element:\n",
    "            details['director'] = director_element.text.strip()\n",
    "        \n",
    "        # Dura√ß√£o\n",
    "        duration_element = soup.find('li', {'data-testid': 'title-techspec_runtime'})\n",
    "        if duration_element:\n",
    "            details['duration'] = duration_element.find('div').text.strip()\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair detalhes do filme: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "print(\"Fun√ß√µes de web scraping definidas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Executando o Web Scraping\n",
    "\n",
    "**Aten√ß√£o:** O web scraping pode levar alguns minutos para ser conclu√≠do, pois fazemos uma pausa entre cada requisi√ß√£o para respeitar o servidor do IMDb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o web scraping\n",
    "print(\"Iniciando web scraping dos top 250 filmes do IMDb...\")\n",
    "print(\"Este processo pode levar alguns minutos...\")\n",
    "\n",
    "movies_data = scrape_imdb_top250()\n",
    "\n",
    "if movies_data:\n",
    "    print(f\"\\nWeb scraping conclu√≠do! {len(movies_data)} filmes extra√≠dos.\")\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    \n",
    "    # Salvar em CSV\n",
    "    df.to_csv('imdb_top250_enhanced.csv', index=False, sep=';')\n",
    "    print(\"Dados salvos em 'imdb_top250_enhanced.csv'\")\n",
    "    \n",
    "    # Mostrar primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nShape do dataset: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"Erro no web scraping. Verifique sua conex√£o com a internet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Carregando Dataset (Alternativa)\n",
    "\n",
    "Caso o web scraping n√£o funcione ou voc√™ queira usar dados pr√©-existentes, pode carregar o dataset do arquivo CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset (descomente se necess√°rio)\n",
    "# df = pd.read_csv('imdb_top250_enhanced.csv', sep=';')\n",
    "\n",
    "# Verificar se o DataFrame foi criado\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame n√£o encontrado. Execute a c√©lula anterior ou carregue um arquivo CSV.\")\n",
    "else:\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "    print(\"\\nPrimeiras linhas:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria dos Dados (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informa√ß√µes b√°sicas do dataset\n",
    "print(\"=== INFORMA√á√ïES B√ÅSICAS DO DATASET ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Colunas: {list(df.columns)}\")\n",
    "print(\"\\n=== TIPOS DE DADOS ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== VALORES NULOS ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n=== ESTAT√çSTICAS DESCRITIVAS ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza e prepara√ß√£o dos dados\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Converter tipos de dados\n",
    "df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce')\n",
    "df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
    "\n",
    "# Remover linhas com valores cr√≠ticos nulos\n",
    "df_clean = df_clean.dropna(subset=['title_en', 'sinopse'])\n",
    "\n",
    "# Preencher valores nulos em g√™nero com 'Unknown'\n",
    "df_clean['genre'] = df_clean['genre'].fillna('Unknown')\n",
    "\n",
    "print(f\"Dataset ap√≥s limpeza: {df_clean.shape}\")\n",
    "print(f\"Valores nulos restantes:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes explorat√≥rias\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Distribui√ß√£o de g√™neros\n",
    "plt.subplot(2, 3, 1)\n",
    "genre_counts = df_clean['genre'].value_counts().head(10)\n",
    "plt.bar(range(len(genre_counts)), genre_counts.values)\n",
    "plt.xticks(range(len(genre_counts)), genre_counts.index, rotation=45, ha='right')\n",
    "plt.title('Top 10 G√™neros')\n",
    "plt.ylabel('Quantidade de Filmes')\n",
    "\n",
    "# 2. Distribui√ß√£o de anos\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df_clean['year'].dropna(), bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Anos de Lan√ßamento')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "\n",
    "# 3. Distribui√ß√£o de ratings\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df_clean['rating'].dropna(), bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "\n",
    "# 4. Rating vs Ano\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df_clean['year'], df_clean['rating'], alpha=0.6)\n",
    "plt.title('Rating vs Ano de Lan√ßamento')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Rating')\n",
    "\n",
    "# 5. Top 10 filmes por rating\n",
    "plt.subplot(2, 3, 5)\n",
    "top_movies = df_clean.nlargest(10, 'rating')[['title_en', 'rating']]\n",
    "plt.barh(range(len(top_movies)), top_movies['rating'])\n",
    "plt.yticks(range(len(top_movies)), [title[:30] + '...' if len(title) > 30 else title for title in top_movies['title_en']])\n",
    "plt.title('Top 10 Filmes por Rating')\n",
    "plt.xlabel('Rating')\n",
    "\n",
    "# 6. Filmes por d√©cada\n",
    "plt.subplot(2, 3, 6)\n",
    "df_clean['decade'] = (df_clean['year'] // 10) * 10\n",
    "decade_counts = df_clean['decade'].value_counts().sort_index()\n",
    "plt.bar(decade_counts.index, decade_counts.values)\n",
    "plt.title('Filmes por D√©cada')\n",
    "plt.xlabel('D√©cada')\n",
    "plt.ylabel('Quantidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©-processamento de Texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar stopwords do NLTK\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"Stopwords em ingl√™s carregadas com sucesso!\")\n",
    "except:\n",
    "    print(\"Erro ao carregar stopwords. Usando lista b√°sica.\")\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
    "\n",
    "# Fun√ß√£o para limpeza de texto\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para limpar e pr√©-processar texto\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == 'N/A':\n",
    "        return \"\"\n",
    "    \n",
    "    # Converter para min√∫sculas\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remover caracteres especiais e n√∫meros\n",
    "    import re\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar limpeza nas sinopses\n",
    "print(\"Aplicando limpeza de texto nas sinopses...\")\n",
    "df_clean['sinopse_clean'] = df_clean['sinopse'].apply(clean_text)\n",
    "\n",
    "# Remover filmes sem sinopse v√°lida\n",
    "df_clean = df_clean[df_clean['sinopse_clean'].str.len() > 10]\n",
    "\n",
    "print(f\"Dataset ap√≥s limpeza de texto: {df_clean.shape}\")\n",
    "print(f\"Exemplo de sinopse limpa:\")\n",
    "print(df_clean['sinopse_clean'].iloc[0][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar TF-IDF\n",
    "print(\"Aplicando TF-IDF nas sinopses...\")\n",
    "\n",
    "# Configurar TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limitar n√∫mero de features\n",
    "    min_df=2,           # Palavra deve aparecer em pelo menos 2 documentos\n",
    "    max_df=0.8,         # Palavra n√£o pode aparecer em mais de 80% dos documentos\n",
    "    ngram_range=(1, 2)  # Usar unigramas e bigramas\n",
    ")\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(df_clean['sinopse_clean'])\n",
    "\n",
    "print(f\"Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
    "print(f\"N√∫mero de features (palavras): {X_tfidf.shape[1]}\")\n",
    "print(f\"N√∫mero de documentos (filmes): {X_tfidf.shape[0]}\")\n",
    "\n",
    "# Mostrar algumas palavras mais importantes\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nPrimeiras 20 features: {feature_names[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo KMeans com k=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo KMeans com k=5\n",
    "print(\"Treinando modelo KMeans com k=5...\")\n",
    "\n",
    "# Configurar e treinar o modelo\n",
    "kmeans = KMeans(\n",
    "    n_clusters=5,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "cluster_labels = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Adicionar labels dos clusters ao DataFrame\n",
    "df_clean['cluster'] = cluster_labels\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "print(f\"Distribui√ß√£o dos clusters:\")\n",
    "print(df_clean['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Calcular silhouette score\n",
    "silhouette_avg = silhouette_score(X_tfidf, cluster_labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lise dos Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o dos clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 1. Distribui√ß√£o dos clusters\n",
    "plt.subplot(2, 2, 1)\n",
    "cluster_counts = df_clean['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_counts.index, cluster_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Distribui√ß√£o dos Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('N√∫mero de Filmes')\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    plt.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 2. G√™neros por cluster\n",
    "plt.subplot(2, 2, 2)\n",
    "# Criar tabela de conting√™ncia\n",
    "genre_cluster = pd.crosstab(df_clean['genre'], df_clean['cluster'])\n",
    "# Normalizar por cluster\n",
    "genre_cluster_pct = genre_cluster.div(genre_cluster.sum(axis=0), axis=1) * 100\n",
    "# Plotar apenas os g√™neros mais comuns\n",
    "top_genres = df_clean['genre'].value_counts().head(5).index\n",
    "genre_cluster_pct.loc[top_genres].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Distribui√ß√£o de G√™neros por Cluster')\n",
    "plt.xlabel('G√™nero')\n",
    "plt.ylabel('Percentual (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# 3. Rating m√©dio por cluster\n",
    "plt.subplot(2, 2, 3)\n",
    "rating_by_cluster = df_clean.groupby('cluster')['rating'].mean()\n",
    "plt.bar(rating_by_cluster.index, rating_by_cluster.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Rating M√©dio por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Rating M√©dio')\n",
    "for i, v in enumerate(rating_by_cluster.values):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Ano m√©dio por cluster\n",
    "plt.subplot(2, 2, 4)\n",
    "year_by_cluster = df_clean.groupby('cluster')['year'].mean()\n",
    "plt.bar(year_by_cluster.index, year_by_cluster.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Ano M√©dio por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Ano M√©dio')\n",
    "for i, v in enumerate(year_by_cluster.values):\n",
    "    plt.text(i, v + 1, f'{v:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada de cada cluster\n",
    "print(\"=== AN√ÅLISE DETALHADA DOS CLUSTERS ===\\n\")\n",
    "\n",
    "for cluster_id in sorted(df_clean['cluster'].unique()):\n",
    "    cluster_data = df_clean[df_clean['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"--- CLUSTER {cluster_id} ---\")\n",
    "    print(f\"N√∫mero de filmes: {len(cluster_data)}\")\n",
    "    print(f\"Rating m√©dio: {cluster_data['rating'].mean():.2f}\")\n",
    "    print(f\"Ano m√©dio: {cluster_data['year'].mean():.0f}\")\n",
    "    \n",
    "    # G√™neros mais comuns\n",
    "    top_genres = cluster_data['genre'].value_counts().head(3)\n",
    "    print(f\"G√™neros mais comuns: {', '.join([f'{genre} ({count})' for genre, count in top_genres.items()])}\")\n",
    "    \n",
    "    # Filmes mais bem avaliados\n",
    "    top_movies = cluster_data.nlargest(3, 'rating')[['title_en', 'rating', 'year']]\n",
    "    print(\"Filmes mais bem avaliados:\")\n",
    "    for _, movie in top_movies.iterrows():\n",
    "        print(f\"  - {movie['title_en']} ({movie['year']}) - Rating: {movie['rating']}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜò Guia de Solu√ß√£o de Problemas\n",
    "\n",
    "### **Problema: N√£o consigo ver os prints**\n",
    "\n",
    "**Solu√ß√µes em ordem de prioridade:**\n",
    "\n",
    "1. **Reiniciar o Kernel**\n",
    "   - V√° em `Kernel` ‚Üí `Restart & Clear Output`\n",
    "   - Execute as c√©lulas novamente\n",
    "\n",
    "2. **Verificar o Ambiente**\n",
    "   - Certifique-se de que est√° usando Jupyter Notebook ou Jupyter Lab\n",
    "   - Verifique se o kernel Python est√° ativo (canto superior direito)\n",
    "\n",
    "3. **Usar as Fun√ß√µes de Display**\n",
    "   - Use `print_and_display()` em vez de `print()`\n",
    "   - Use `display_df()` para DataFrames\n",
    "\n",
    "4. **Verificar Configura√ß√µes**\n",
    "   - Execute a c√©lula de diagn√≥stico no in√≠cio do notebook\n",
    "   - Verifique se todas as bibliotecas foram importadas corretamente\n",
    "\n",
    "5. **Alternativas**\n",
    "   - Use `display()` do IPython\n",
    "   - Use `IPython.display.HTML()` para textos formatados\n",
    "   - Salve os resultados em arquivos CSV e abra externamente\n",
    "\n",
    "### **Problema: Erro de importa√ß√£o de bibliotecas**\n",
    "\n",
    "**Solu√ß√µes:**\n",
    "- Execute: `!pip install [nome_da_biblioteca]`\n",
    "- Reinicie o kernel ap√≥s instala√ß√£o\n",
    "- Verifique se est√° no ambiente correto\n",
    "\n",
    "### **Problema: Web scraping n√£o funciona**\n",
    "\n",
    "**Solu√ß√µes:**\n",
    "- Verifique sua conex√£o com a internet\n",
    "- Use a vers√£o alternativa com dados simulados\n",
    "- Execute o c√≥digo em hor√°rios de menor tr√°fego\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Nuvem de Palavras por Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar nuvem de palavras por cluster\n",
    "def create_wordcloud_for_cluster(cluster_id, df_data, column='sinopse_clean'):\n",
    "    \"\"\"\n",
    "    Cria nuvem de palavras para um cluster espec√≠fico\n",
    "    \"\"\"\n",
    "    cluster_data = df_data[df_data['cluster'] == cluster_id]\n",
    "    text = ' '.join(cluster_data[column].astype(str))\n",
    "    \n",
    "    if len(text.strip()) == 0:\n",
    "        print(f\"Cluster {cluster_id}: Sem texto dispon√≠vel\")\n",
    "        return\n",
    "    \n",
    "    # Criar nuvem de palavras\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate(text)\n",
    "    \n",
    "    # Plotar\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Cluster {cluster_id} - Palavras Mais Frequentes\\n({len(cluster_data)} filmes)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# Criar nuvens de palavras para cada cluster\n",
    "print(\"Criando nuvens de palavras para cada cluster...\")\n",
    "for cluster_id in sorted(df_clean['cluster'].unique()):\n",
    "    create_wordcloud_for_cluster(cluster_id, df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclus√µes e Insights\n",
    "\n",
    "### 7.1 An√°lise dos Resultados do Modelo KMeans (k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights e Conclus√µes:**\n",
    "\n",
    "1. **Distribui√ß√£o dos Clusters:**\n",
    "   - O modelo KMeans com k=5 conseguiu agrupar os filmes de forma relativamente equilibrada\n",
    "   - Cada cluster representa um perfil distinto de filmes baseado nas sinopses\n",
    "\n",
    "2. **Caracter√≠sticas dos Clusters:**\n",
    "   - **Cluster 0:** [Descrever caracter√≠sticas baseadas na an√°lise]\n",
    "   - **Cluster 1:** [Descrever caracter√≠sticas baseadas na an√°lise]\n",
    "   - **Cluster 2:** [Descrever caracter√≠sticas baseadas na an√°lise]\n",
    "   - **Cluster 3:** [Descrever caracter√≠sticas baseadas na an√°lise]\n",
    "   - **Cluster 4:** [Descrever caracter√≠sticas baseadas na an√°lise]\n",
    "\n",
    "3. **Padr√µes Identificados:**\n",
    "   - [Identificar padr√µes nos g√™neros, anos, ratings por cluster]\n",
    "   - [Analisar se h√° correla√ß√£o entre caracter√≠sticas dos filmes e clusters]\n",
    "\n",
    "4. **Qualidade do Modelo:**\n",
    "   - Silhouette Score: [Valor obtido]\n",
    "   - [Avaliar se o score indica boa separa√ß√£o dos clusters]\n",
    "\n",
    "5. **Aplica√ß√£o Pr√°tica:**\n",
    "   - O modelo pode ser usado para recomendar filmes similares baseados nas sinopses\n",
    "   - Cada cluster representa um \"perfil\" de filme que pode ser usado para personaliza√ß√£o\n",
    "\n",
    "**Limita√ß√µes:**\n",
    "- O modelo considera apenas as sinopses, ignorando outras caracter√≠sticas importantes\n",
    "- A qualidade das sinopses extra√≠das pode variar\n",
    "- Alguns filmes podem n√£o se encaixar perfeitamente em nenhum cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados finais\n",
    "print(\"Salvando resultados finais...\")\n",
    "\n",
    "# Salvar DataFrame com clusters\n",
    "df_clean.to_csv('imdb_top250_with_clusters.csv', index=False, sep=';')\n",
    "print(\"Dataset com clusters salvo em 'imdb_top250_with_clusters.csv'\")\n",
    "\n",
    "# Salvar resumo dos clusters\n",
    "cluster_summary = df_clean.groupby('cluster').agg({\n",
    "    'title_en': 'count',\n",
    "    'rating': 'mean',\n",
    "    'year': 'mean',\n",
    "    'genre': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "}).round(2)\n",
    "\n",
    "cluster_summary.columns = ['Num_Filmes', 'Rating_Medio', 'Ano_Medio', 'Genero_Principal']\n",
    "cluster_summary.to_csv('cluster_summary.csv', sep=';')\n",
    "print(\"Resumo dos clusters salvo em 'cluster_summary.csv'\")\n",
    "\n",
    "print(\"\\n=== RESUMO FINAL ===\")\n",
    "print(f\"Total de filmes analisados: {len(df_clean)}\")\n",
    "print(f\"N√∫mero de clusters: {df_clean['cluster'].nunique()}\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(\"\\nDistribui√ß√£o dos clusters:\")\n",
    "print(df_clean['cluster'].value_counts().sort_index())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
